{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you lost points on the last checkpoint you can get them back by responding to TA/IA feedback**  \n",
    "\n",
    "Update/change the relevant sections where you lost those points, make sure you respond on GitHub Issues to your TA/IA to call their attention to the changes you made here.\n",
    "\n",
    "Please update your Timeline... no battle plan survives contact with the enemy, so make sure we understand how your plans have changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - EDA Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Benjamin Xia\n",
    "- Kailey Wong\n",
    "- Jesus Tello\n",
    "- Thor\n",
    "- Wasp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much of an impact do cryptocurrency miners/prices actually have on GPU prices and stock for individual customers (e.g. gamers)? Are there any other factors that have had a larger impact (e.g. global chip shortages affecting GPU's and other kinds of chips)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Graphics Processing Units (a.k.a. GPUs, graphics cards, video cards) allow computers to perform parallel computations on a scale that is simply impossible on most traditional processing units (central processing units, a.k.a. CPUs). GPUs have allowed faster data processing, rendering, training for machine learning algorithms, and more. Gaming on a low-spec potato laptop often hinders the smooth play of demanding triple-A titles due to hardware limitations. To cope, users must reduce graphics settings, sacrificing visual quality for better performance. However, some games may still remain unplayable, necessitating the exploration of less demanding titles or an upgrade to a more powerful system. Many game developers are even expecting players to have powerful machines as an excuse not to optimize their games to perform well on lower-end machines properly.<a name=\"cite_ref-1\"></a>[<sup>1</sup>](#cite_note-1) As computer graphics continue to progress and games become more graphically intensive, demand for consumer-grade GPUs such as NVIDIA's Geforce lineup has only grown.\n",
    "\n",
    "When cryptocurrency became mainstream (around 2017), more people started mining cryptocurrencies with consumer-grade GPUs due to their unreasonable effectiveness compared to traditional processors and the lack of supply for ASIC (Application-Specific Integrated Circuit) units specialized for cryptocurrency mining.<a name=\"cite_ref-2\"></a>[<sup>2</sup>](#cite_note-2) Cryptocurrency miners, along with the onset of the pandemic led to GPU supplies dwindling, leading gamers to start blaming cryptocurrency miners for GPU shortages and sky-high pricing by eBay scalpers. Many GPU models, such as the NVIDIA GTX 1000 series and RTX 3000 series, were unavailable at MSRP (Manufacturer's Suggested Retail Price) for months. Gamers' reaction to the GPU market is evident by overall sentiment on online communities such as Reddit.<a name=\"cite_ref-3\"></a>[<sup>3</sup>](#cite_note-3) Vendors have attempted to allow more individual customers to get their hands on these precious GPU's by imposing purchase limits.<a name=\"cite_ref-4\"></a>[<sup>4</sup>](#cite_note-4)\n",
    "\n",
    "The story of how GPU prices skyrocketed has since become a case study in how supply and demand can swiftly decimate a consumer market.<a name=\"cite_ref-5\"></a>[<sup>5</sup>](#cite_note-5)\n",
    "\n",
    "1. <a name=\"cite_note-1\"></a> [^](#cite_ref-1) Crider, Michael (7 Sep 2023) Pre-Crypto Prices When? *PCWorld*. https://www.pcworld.com/article/2058969/trouble-running-starfield-todd-howard-says-upgrade-your-pc.html\n",
    "2. <a name=\"cite_note-2\"></a> [^](#cite_ref-2) Iyer, S.G., Pawar, A. Dipakumar. (28 Feb 2019) GPU and CPU Accelerated Mining of Cryptocurrencies and their Financial Analysis *IEEE*. https://ieeexplore.ieee.org/document/8653733\n",
    "3. <a name=\"cite_note-3\"></a> [^](#cite_ref-3) u/rcmaehl (18 Nov 2018) Pre-Crypto Prices When? *r/pcmasterrace*. https://www.reddit.com/r/pcmasterrace/comments/9ygmux/precrypto_prices_when/\n",
    "4. <a name=\"cite_note-4\"></a> [^](#cite_ref-4) Dent, Steve (11 Feb 2022) Best Buy Limits Sales of NVIDIA RTX-Series GPUs to Totaltech Subscribers. *EnGadget*. https://www.engadget.com/best-buy-gpu-sales-totaltech-membership-paywall-092357559.html\n",
    "5. <a name=\"cite_note-5\"></a> [^](#cite_ref-5) Lim, H.W., Wibowo, T. (12 Apr 2022) Cryptocurrency Mining Effects On Semiconductor Shortage on PC Owner Community. *CoMBInES - Conference On Management, Business, Innovation, Education And Social Sciences*. https://journal.uib.ac.id/index.php/combines/article/view/6634"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We suspect that cryptocurrency miners do have some impact on GPU prices and supply (in that there is some positive correlation between cryptocurrency prices and GPU prices), though its impact is often exaggerated by gamers on online communities as external factors such as chip (semiconductor) shortages often have a greater impact on price and supply. We believe this to be the case because many vendors often place limits on how many GPU's customers can purchase at at a time, and many of the GPU shortages have happened to coincide with global chip shortages (that were not isolated to GPU's)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data overview\n",
    "\n",
    "For each dataset include the following information\n",
    "- Dataset #1\n",
    "  - Dataset Name: Steam Hardware Surveys\n",
    "  - Link to the dataset: https://raw.githubusercontent.com/jdegene/steamHWsurvey/master/shs.csv\n",
    "  - Number of observations: 9543\n",
    "  - Number of variables: 5\n",
    "  \n",
    "Each observation in this data set has a date for the date of the survey response, category for the type of hardware the entry is for, the name of the particular product, the percentage change in popularity from the previous month (negative values indicate less popularity, positive more popularity), and the percentage of total items in the category that the particular product makes up. The date, category, and name are strings, and the change and percentage are floats. This data will help us to understand the usage of particular GPU products over time. To wrangle and clean the dataset, we will need to filter the data by dates and the hardware category we are interested in, then check for and drop rows with missing data.\n",
    "\n",
    "- Dataset #2\n",
    "  - Dataset Name: Bitcoin USD Historical Data\n",
    "  - Link to the dataset: https://finance.yahoo.com/quote/BTC-USD/history?period1=1420070400&period2=1698796800&interval=1mo&filter=history&frequency=1mo&includeAdjustedClose=true\n",
    "  - Number of observations: 107\n",
    "  - Number of variables: 7 \n",
    "  \n",
    "Each observation in this data set has a date to indicate what month/year the data corresponds to, the opening price, the highest price during that month, the lowest price during that month, the closing price for that month, an adjusted closing price for splits/dividend/capital gain distributions (all in USD/floats), and volume of stocks traded during that month (integers). This data will help us to understand how bitcoin prices fluctuate overtime. We are able to preselect the dates we are interested in when downloading the csv for the data, so to wrangle and clean the dataset, we only need to check for missing data, and standardize the column headers to make it easier to combine datasets later on.\n",
    "\n",
    "- Dataset #3\n",
    "    - Dataset Name: Ethereum USD Historical Data\n",
    "  - Link to the dataset: https://finance.yahoo.com/quote/ETH-USD/history?period1=1510185600&period2=1698796800&interval=1mo&filter=history&frequency=1mo&includeAdjustedClose=true\n",
    "  - Number of observations: 72\n",
    "  - Number of variables: 7\n",
    "  \n",
    "Same as for bitcoin, each observation in this data set has a date to indicate what month/year the data corresponds to, the opening price, the highest price during that month, the lowest price during that month, the closing price for that month, an adjusted closing price for splits/dividend/capital gain distributions (all in USD/floats), and volume of stocks traded during that month (integers). This data will serve as another proxy to help us to understand how bitcoin prices fluctuate overtime. We are able to preselect the dates we are interested in when downloading the csv for the data, so to wrangle and clean the dataset, we only need to check for missing data, and standardize the column headers to make it easier to combine datasets later on (being able to distinguish between BTC vs ETH data).\n",
    "\n",
    "- Dataset #4\n",
    "  - Dataset Name: Historical CPU sell price/sell volume\n",
    "  - Link to the dataset: Scraped from Ebay Terapeak: https://www.ebay.com/help/selling/selling-tools/terapeak-research?id=4853\n",
    "  - Number of observations: 18527\n",
    "  - Number of variables: 4\n",
    "\n",
    "This was scraped from Ebay Terapeak using the script in our repository `ebay_scraper.py`. Each observation has a UNIX timestamp (which could easily be converted to month/year), model name, along with that CPU model's average sell price each week (in USD) and average count of units sold that week. This data will serve as a proxy to help us understand how GPU and CPU prices and supply have changed over time, and we will take a look at how they relate to each other (CPU vs. GPU prices), and cryptocurrency prices over time. The data we collected is (mostly) clean, though we do need to remove observations that come before a model was released, as an Intel 13700k would obviously have 0 units sold in 2020 as it wasn't even released then. The datatypes are integer timestamps, model names as strings, sell price in USD (integers, though Pandas will interpret them as floats by default), and units sold as integers. \n",
    "\n",
    "- Dataset #5\n",
    "  - Dataset Name: Historical GPU sell price/sell volume\n",
    "  - Link to the dataset: Scraped from Ebay Terapeak: https://www.ebay.com/help/selling/selling-tools/terapeak-research?id=4853\n",
    "  - Number of observations: 6438\n",
    "  - Number of variables: 4\n",
    "\n",
    "Identical format to dataset #4.\n",
    "This was scraped from Ebay Terapeak using the script in our repository `ebay_scraper.py`. Each observation has a UNIX timestamp (which could easily be converted to month/year), model name, along with that GPU model's average sell price each week (in USD) and average count of units sold that week. This data will serve as a proxy to help us understand how GPU and CPU prices and supply have changed over time, and we will take a look at how they relate to each other (CPU vs. GPU prices), and cryptocurrency prices over time. The data we collected is (mostly) clean, though we do need to remove observations that come before a model was released, as an RTX 4080 would obviously have 0 units sold in 2020 as it wasn't even released then. The datatypes are integer timestamps, model names as strings, sell price in USD (integers, though Pandas will interpret them as floats by default), and units sold as integers. \n",
    "\n",
    "\n",
    "To combine these datasets, we will take their intersection based on the dates. We may need to combine observations from our CPU and GPU data to be monthly in order to accomplish this (though this may not be completely necessary). Once the data is all \"synced\" up, we can combine different kinds of data into a signle table for finer-grained analysis of different variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset #1 (use name instead of number here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n",
    "## FEEL FREE TO ADD MULTIPLE CELLS PER SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset #2 (if you have more than one, use name instead of number here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n",
    "## FEEL FREE TO ADD MULTIPLE CELLS PER SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "## Exploratory Data Analysis\n",
    "\n",
    "Carry out whatever EDA you need to for your project.  Because every project will be different we can't really give you much of a template at this point. But please make sure you describe the what and why in text here as well as providing interpretation of results and context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1 of EDA - please give it a better title than this\n",
    "\n",
    "Some more words and stuff.  Remember notebooks work best if you interleave the code that generates a result with properly annotate figures and text that puts these results into context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "## FEEL FREE TO ADD MULTIPLE CELLS PER SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2 of EDA if you need it  - please give it a better title than this\n",
    "\n",
    "Some more words and stuff.  Remember notebooks work best if you interleave the code that generates a result with properly annotate figures and text that puts these results into context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "## FEEL FREE TO ADD MULTIPLE CELLS PER SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data come from Steam surveys, eBay sales, and general finance data. All of these sources are easily-accessible and public information. Because the data from the Steam surveys is already anonymized, there should not be any ethical or privacy concerns with being able to extrapolate personal identifiable information from the survey data. However, this makes it difficult to account for any potential biases in the dataset as we can only guess at the demographic of Steam users who answered the survey. Additionally, in these types of surveys it is important to be aware of many different types of biases that may come into play, including non-response, self-reporting, and other confounding variables, which could lead to a disproportionate amount of high-end hardware appearing in survey results. When choosing to scrape data from eBay, we had to be aware of terms of service and other restrictions before choosing this as a source for our final dataset. Regarding eBay sales and finance data, we donâ€™t anticipate any ethical or privacy related concerns in our use of the data since by nature it is anonymized. There is potential to be able to identify individual companies or investors through the data, one way to address this is simply aggregating data, since we only require a broader overview. There may be concerns as to how this data is acquired and any potential biases that may be introduced. To help detect these biases as we perform analysis of the data, we will be careful to thoroughly explore the data before deeper analysis, attempting to fully understand it and being careful to note factors that may indicate biases in the data. In our post-analysis, we will discuss the results of our exploration. If we suspect any biases, we will explain their significance and the impact on the results of our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Attend weekly meetings on Sunday 9pm (or have a good excuse).\n",
    "* On weeks where some project component is due on Wednesday, attempt to have a working draft (e.g. 90% complete) by Monday, have everyone look over everything and revise on Tuesday, and finalize on Wednesday. Depending on the scale of a project component this timeline may be pushed forward.\n",
    "* If you ghost meetings without saying anything or fail to make meaningful attempts to contribute, your name will not be added to the notebook corresponding to whatever project component is due, and there will be a note that you did not contribute underneath everyone else's names :(\n",
    "* Don't sabotage other people's stuff >:(\n",
    "* Don't plagiarize :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 10/23  | 4:10 PM | Review previous data analysis projects and think about strengths and weaknesses of each project | Discuss about previous projects to avoid making the same mistakes|\n",
    "| 10/30  | 9:00 PM | Brainstorm topics/questions; Review the previous project to get some idea  | Determine best form of communication; Discuss and decide on data analysis topic; discuss hypothesis; begin background research |\n",
    "| 11/05  | 9:00 PM |Finalize datasets we will use for our project | Discuss about data wrangling and possible analytical approaches; Descriptive Analysis for checkpoint 1 | \n",
    "| 11/12  |  9:00 PM |  Data Cleaning and preprocessing; Explores the data | draft data checkpoint;Assign group members to lead each specific part | \n",
    "| 11/19  | 9:00 PM  | Finalize and submit data checkpoint; Import & Wrangle Data; EDA  | Review/Edit wrangling/EDA; Discuss Analysis Plan; Draft EDA Checkpoint;   |\n",
    "| 11/26  |9:00 PM  | Finalize EDA Checkpoint and submit; Begin Anlysis| Discuss/edit Analysis  |\n",
    "| 12/03  | 9:00 PM  | Complete analysis; Draft results/conclusion/discussion; | Discuss/edit final project |\n",
    "| 12/10  | 9:00 PM  | Make demo video| Turn in Final Project & Video |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
